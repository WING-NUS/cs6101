---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
header:
  overlay_image: unsplash-t2510.jpg
#  overlay_image: ricardo-gomez-angel-9AdeEdYB2yk-unsplash.jpg
#  overlay_filter: rgba(115, 74, 2, 0.5)
  overlay_filter: rgba(115, 74, 2, 0.5)
#  caption: "Photo credit: Un @ [**Unsplash**](https://unsplash.com/@rgaleriacom)"
---

{% include base_path %}

Here we archive and cross link all of the past projects done by our first year Ph.D. students, undergraduate and external guests that take part in our reading group.  Generally, when students and participants outside of WING join the reading group they must also complete a related project touching on some part of the lecture topics.  These projects often get presented publicly in the forum of our School of Computing's Term Project Showcase (STePS).

## Table of Contents

### Volume 2025 (Semester 2510 (AY 25/26, Sem I) featured at 27th STePS, held on 12 Nov 2025)

**Project 01:** CompRAG: Retrieval for Multiple Hops [ [Gallery](#project-2025-01) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-01.jpg) ]  
*Indraneel Paranjape, Nayanthara Prathap, Nura Tamton, Vangmay Sachan*

**Project 02:** Mixture-of-LoRA-Experts for Continual Learning in Generative Retrieval [ [Gallery](#project-2025-02) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-02.jpg) ]  
*Benjamin Chek, Choong Kai Zhe, Zak Tng*

**Project 03:** SLM + RAG [ [Gallery](#project-2025-03) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-03.jpg) ]  
*Jonathan Chen, Shyamal Narang, JF Koh*

**Project 04:** Cutting Redundant Knowledge for Effective RAG [ [Gallery](#project-2025-04) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-04.jpg) ]  
*Mingyu Lee, Swislar Tan, Ervin Teo*

**Project 05:** ScholarAI:Simplifying Interdisciplinary AI Research Using GraphRAG [ [Gallery](#project-2025-05) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-05.jpg) ]  
*Rishav Ghosh, Zaidan Sani, Nicholas Cheng, Qianbo Dong*

**Project 06:** MathRAG: Retrieval-Augmented Generation for Verifying Math Solutions. [ [Gallery](#project-2025-06) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-06.jpg) ]  
*Kseniia Petukhova, Van-Hoang Nguyen*

**Project 07:** Fine-Grained Multimodal RAG: Enhancing Retrieval with Object-Level Representations [ [Gallery](#project-2025-07) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-07.jpg) ]  
*Manaswini Talagadadivi, Shen Ting Ang, Huang Chao Ming, Benjamin Goh, Estelle Sim*

**Project 09:** Automatic Accelerator Code Generation via LLM Agentic Workflow [ [Gallery](#project-2025-09) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-09.jpg) ]  
*Takanori Aoki*

**Project 10:** When Retrieval Misleads: Exploring Vulnerabilities in RAG [ [Gallery](#project-2025-10) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-10.jpg) ]  
*Sahej Agarwal, Jundong Xu, Ruiwen Zhou, Sahajpreet Singh*

**Project 11:** Contexts Ground Discourse [ [Gallery](#project-2025-11) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-11.jpg) ]  
*Yisong Miao*

**Project 12:** Context-Aware RAG: Enhancing Retrieval with Contextual Information [ [Gallery](#project-2025-12) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-12.jpg) ]  
*Xinpeng Liu, Ng Xuan Hern, Oshan Jayawardena*

**Project 13:** Improving Retrieval with Graph Pruning [ [Gallery](#project-2025-13) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-13.jpg) ]  
*Aaron Toh, Frederick Amal Emerson, Hong Yi, Yong Ee*

**Project 14:** A Unified Benchmark Suite for Retrieval-Augmented Speculative Decoding [ [Gallery](#project-2025-14) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-14.jpg) ]  
*Ada Ho, Tan Chien Hao, Lee Kwan Tze, Benn Tan*

**Project 16:** Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation [ [Gallery](#project-2025-16) ] [ [Poster](/cs6101/files/STePS-2025/CS6101-16.jpg) ]  
*Jiaying Wu, Zihang Fu, Haonan Wang, Fanxiao Li, Min-Yen Kan*

---

### Volume 2023 (Semester 2310 (AY 23/24, Sem I) featured at 23th STePS, held on 15 Nov 2023)

**Project 02:** AugICL [ [Gallery](#project-2023-02) ] [ [Poster](/cs6101/files/STePS-2023/CS6101-02.png) ]  
*Xiachong Feng, Yisong Miao*

**Project 05:** Chain of Action [ [Gallery](#project-2023-05) ] [ [Poster](/cs6101/files/STePS-2023/CS6101-05.jpeg) ]  
*Nicholas Wong, Richmond Sin, Ellawela Suveen Thinusha Bandara, Low Keng Hoong (Warren), Gan Kah Ee*

**Project 06:** Exploring self-supervised webscraper code generation with LLMs [ [Gallery](#project-2023-06) ] [ [Poster](/cs6101/files/STePS-2023/CS6101-06.png) ]  
*Arnav Aggarwal, Filbert Phang Kong San, Soon Kang Le (Conrad), Wee Yen Zhe, Alson Jiang*

**Project 09:** LLM for Table Fact-Checking & Reasoning [ [Gallery](#project-2023-09) ] [ [Poster](/cs6101/files/STePS-2023/CS6101-09.png) ]  
*Naomi Leow, Xinyuan LU*

**Project 14:** Textualization of Visual Information [ [Gallery](#project-2023-14) ] [ [Poster](/cs6101/files/STePS-2023/CS6101-14.png) ]  
*Xiao Xu*

---


### Volume 2021 (Semester 2020 (AY 20/21, Sem II) featured at 18th STePS, held on 14 Apr 2021)

**Project 01:** Explore Multiple Response Modalities of DialogWAE [ [Gallery](#project-2021-01) ] [ [Poster](https://postimg.cc/sQN3hscm) ]  
*Liu Ruofan, Liu Hongfu, Liu Yong*

**Project 02:** FiBiNET [ [Gallery](#project-2021-02) ] [ [Poster](https://raw.githubusercontent.com/lizihan97/FiBiNET/main/FiBiNET.png) ]  
*Qiao Rui, Sng Weicong, Li Zihan*

**Project 03:** Feedback-guided Preference Adaptation Network (FPAN) [ [Gallery](#project-2021-03) ] [ [Poster](https://drive.google.com/file/d/1SoAqjjmq48rMtvpQvCt-Wne5dXmiXOkA/view) ]  
*Henry Kasim, Samuel Broughton*

**Project 04:** Causal Estimation for Conversational Recommender Systems [ [Gallery](#project-2021-04) ] [ [Poster](https://yisong.me/publications/CausalEst-Poster.jpeg) ]  
*Yisong Miao, Chenxin Wang*

**Project 05:** Counterfactual Recommender [ [Gallery](#project-2021-05) ] [ [Poster](https://drive.google.com/file/d/1nZ40KXyccOxUJRD53c0m2VcWtBiVWkBb/view) ]  
*Aadit Rahul Kamat, Takanori Aoki*

**Project 06:** Diversifying Dialogue Generation with Non-Conversational Text [ [Gallery](#project-2021-06) ] [ [Poster](https://i.imgur.com/J3tzCR5.png) ]  
*Yeo Qi Xun, Yuxi Xie, Tian Zhen*

**Project 07:** Extending Neural Collaborative Filtering [ [Gallery](#project-2021-07) ] [ [Poster](https://raw.githubusercontent.com/gabrielloye/neural_collaborative_filtering/master/assets/CS6101_Neural_Collaborative_Filtering_Poster.png) ]  
*Gabriel Loye, Clarence Ong, Nham Quoc Hung, Sashankh CK*

**Project 08:** NN for Ad Recommendation [ [Gallery](#project-2021-08) ] [ [Poster](https://i.ibb.co/vVzJdLq/WING-Poster-STe-PS-2021-6101-08.jpg) ]  
*Muhammad Assyarul Ariffin Bin Omar, Lee Xiong An, Xu Pengtai*

**Project 09:** Beyond IGMC [ [Gallery](#project-2021-09) ] [ [Poster](https://drive.google.com/file/d/1Hso_rqGMXnJsRt1V9QmLUUGgbO6OcvZI/view) ]  
*Stephen Tan, Axel Lau Wei En, Joel Tan Wan Rong, Wendi Ren, Chan Guan Hao*

**Project 10:** KGRecSys [ [Gallery](#project-2021-10) ] [ [Poster](https://raw.githubusercontent.com/evantkchong/cs6101_kgrecsys/main/docs/KGRecSys_poster_STePS_2021.png) ]  
*Evan Chong, Rabiul Awal*



## Projects from Semester 2510 (AY 25/26, Sem I) featured at <a href="https://uvents.nus.edu.sg/event/27th-steps">27th STePS</A>, held on 12 Nov 2025.

<!--Table START-->
<table>
  <thead>
    <tr>
      <th><h1 align="center">Projects</h1></th>
      <th><h1 align="center">Posters</h1></th>
    </tr>
  </thead>
  <tbody>
    <!-- ROW 1: Team 01-->
    <tr id="project-2025-01">
      <td width="70%">
        <h1>
          Team 01: CompRAG: Retrieval for Multiple Hops
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">CompRAG enhances multi-hop RAG by composing entity‚Äìrelation-entity triplets with HRR embeddings, enabling relation-focused retrieval that bridges semantic and graph-based methods.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">CompRAG (COMPrehension and COMPosition RAG) explores how relations in a text can be expressed as composite to enhance multi-hop answering outcomes in RAG. We extract entity-relation-entity triplets from text, but innovate by leveraging Holographic Reduced Representations (HRR) [1] to compose individual triplet vectors together, preserving the direction of relations in vector embeddings. At query time, processed query vectors are matched to the most similar triplets in the index. The associated chunks form the context for the LM. By focusing retrieval on the relations present in the text rather than raw similarity or pure entity-linked graphs, CompRAG aims to bridge semantic and graph methods to improve multi-hop retrieval outcomes.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Indraneel Paranjape, External Guest [&nbsp;<a href="https://www.linkedin.com/in/indraneel-p/">LinkedIn</a>&nbsp;]</li>
          <li>Nayanthara Prathap, External Guest [&nbsp;<a href="https://www.linkedin.com/in/nayanthara-prathap/">LinkedIn</a>&nbsp;]</li>
          <li>Nura Tamton, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/nura-tamton/">LinkedIn</a>&nbsp;]</li>
          <li>Vangmay Sachan, NUS Undergraduate Student [&nbsp;<a href="https://www.linkedin.com/in/vangmay-sachan-3582b5213/">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/indraneelrp/CompRAG" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-01.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 1: Team 07 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-01.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-01.jpg" alt="Team 01 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 2: Team 02-->
    <tr id="project-2025-02">
      <td width="70%">
        <h1>
          Team 02: Mixture-of-LoRA-Experts for Continual Learning in Generative Retrieval
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">This project implements the MixLoRA-DSI pipeline to explore continual learning in generative retrieval systems via the usage of LoRA experts. <BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">This project reproduces the Mixture-of-LoRA-Experts for Generative Retrieval (MixLoRA-DSI) framework to empirically study how parameter-efficient fine-tuning enables continual learning in generative retrieval systems. We implemented the full pipeline‚Äîfrom T5 pretraining under RQ-based DocIDs to constrained decoding‚Äîand replicated key results on MSMARCO and NQ320k datasets. Through this process, we analyzed the interactions between LoRA modules, mixture routing, and residual quantization, identifying challenges in stability, convergence, and data handling. Our findings provide practical insights into implementing rehearsal-free generative retrievers and clarify how modular fine-tuning mechanisms can balance efficiency and plasticity in large-scale language-retrieval architectures.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Benjamin Chek [&nbsp;<a href="https://www.linkedin.com/in/benjamin-chek" style="font-size:18px">LinkedIn</a>&nbsp;]</li>
          <li>Choong Kai Zhe [&nbsp;<a href="https://www.linkedin.com/in/choong-kai-zhe" style="font-size:18px">LinkedIn</a>&nbsp;]</li>
          <li>Zak Tng</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/exxakt/MoLE-for-GR/tree/main" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-02.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 2: Team 02 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-02.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-02.jpg" alt="Team 02 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 3: Team 03-->
    <tr id="project-2025-03">
      <td width="70%">
        <h1>
          Team 03: SLM + RAG
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">The project focuses on study of efficacy of RAG solution built on SLMs for domain specific knowledge task.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">With the rise of Generative AI, Small Language Models (SLMs) offer a more practical and affordable solution for industrial deployment compared to Large Language Models (LLMs). For integrating domain-specific knowledge, Retrieval-Augmented Generation (RAG) is often a more effective strategy than model fine-tuning. This project investigates the efficacy of RAG systems built upon SLMs. We will evaluate their performance and challenges across various question categories using product technical documents. The 2nd objective is to analyse whether the underlying language model size (SLM vs. LLM) significantly impacts the overall performance and reliability of the RAG system.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Jonathan Chen</li>
          <li>Shyamal Narang</li>
          <li>JF Koh</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/jfkoh/nus_wing_rag_team3" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-03.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 3: Team 03 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-03.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-03.jpg" alt="Team 03 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 4: Team 04-->
    <tr id="project-2025-04">
      <td width="70%">
        <h1>
          Team 04: Cutting Redundant Knowledge for Effective RAG
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Evaluating the impact of corpus deduplication (SimHash, MinHash) on RAG retrieval efficiency and downstream model accuracy.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Retrieval systems often return multiple near-duplicate documents reducing retrieval diversity and efficiency. This project addresses that challenge by clustering near-duplicates and selecting representative documents to keep Retrieval-Augmented Generation (RAG) both efficient and relevant. We evaluate two deduplication methods on their impact on RAG performance. Overall, moderate deduplication effectively reduces redundancy without harming performance, suggesting that RAG systems can safely benefit from cleaner, more diverse corpora.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Mingyu Lee [&nbsp;<a href="http://linkedin.com/in/mingyu-lee-329338197">LinkedIn</a>&nbsp;]</li>
          <li>Swislar Tan [&nbsp;<a href="https://www.linkedin.com/in/swislar/">LinkedIn</a>&nbsp;]</li>
          <li>Ervin Teo [&nbsp;<a href="https://www.linkedin.com/in/ervinteo/">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-04.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 4: Team 04 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-04.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-04.jpg" alt="Team 04 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 5: Team 05-->
    <tr id="project-2025-05">
      <td width="70%">
        <h1>
          Team 05: ScholarAI:Simplifying Interdisciplinary AI Research Using GraphRAG
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Enabling easier research by automated ontology extraction to generate knowledge graphs for smarter retrieval.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">This project builds a graph- and ontology-augmented QA system over AI papers from arXiv by combining GraphRAG for entity/relation-aware retrieval with OntoRAG for schema- and rule-driven reasoning. We construct a heterogeneous knowledge graph of papers, authors, methods, datasets, align it with AI ontologies (subfields, method and dataset taxonomies), and use ontological constraints to normalize terms, resolve synonyms, and support multi-hop inference<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>
            Rishav Ghosh, External Guest [&nbsp;<a href="https://www.linkedin.com/in/rishavghosh605/">LinkedIn</a>&nbsp;]
          </li>
          <li>
            Zaidan Sani, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/mzaidanbsani/">LinkedIn</a>&nbsp;]
          </li>
          <li>
            Nicholas Cheng, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/nicholas-cheng-/">LinkedIn</a>&nbsp;]
          </li>
          <li>
            Qianbo Dong, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/qianbodong/">LinkedIn</a>&nbsp;]
          </li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a
            href="https://github.com/rollingpencil/scholar-ai-rag"
            style="font-size: 18px"
            >GitHub Repository</a
        >&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-05.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 5: Team 05 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-05.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-05.jpg" alt="Team 05 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 6: Team 06-->
    <tr id="project-2025-06">
      <td width="70%">
        <h1>
          Team 06: MathRAG: Retrieval-Augmented Generation for Verifying Math Solutions.
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">We enhance step-level mathematical solution verification by augmenting LLM evaluators with external mathematical knowledge via retrieval.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Current evaluations of Large Language Models (LLMs) in mathematical reasoning typically judge only the final answer and often overlook whether the intermediate steps are logically valid. Prior work on LLM-based evaluators also tends to rely solely on the model's internal parametric reasoning ability, which can be inconsistent when verifying multi-step solutions.<br>
        In this project, we introduce <b>MathRAG</b>, a Retrieval-Augmented Generation approach designed to improve step-level mathematical solution verification. Using the Named Mathematical Formulas (NMF) dataset, we construct a dense retrieval system. For each step in a solution, the system retrieves relevant mathematical formulas and appends them as context for a verifier model.<br>
        Building on the ReasonEval framework, we apply this RAG-enhanced context to the ReasonEval-7B verifier for predicting the correctness of step-level reasoning in the PRM800K dataset. Our experiments demonstrate that providing external mathematical formulas improves the verifier's performance, increasing AUC by 3.7% and Macro-F1 by 2.5%. These findings highlight the value of combining external mathematical knowledge with LLM reasoning to achieve more reliable evaluation of multi-step mathematical solutions.
        <BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Kseniia Petukhova [&nbsp;<a href="https://www.linkedin.com/in/kseniia-petukhova/">LinkedIn</a>&nbsp;]</li>
          <li>Van-Hoang Nguyen [&nbsp;<a href="https://www.linkedin.com/in/nguyenvanhoang7398/">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-06.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 6: Team 06 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-06.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-06.jpg" alt="Team 06 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 7: Team 07-->
    <tr id="project-2025-07">
      <td width="70%">
        <h1>
          Team 07: Fine-Grained Multimodal RAG: Enhancing Retrieval with Object-Level Representations
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">This project developed a Fine-Grained Multimodal RAG pipeline that integrates DETR object detection to enhance visual reasoning on the MRAG-Bench through structured text prompts and bounding-box overlays.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Large Vision-Language Models (LVLMs) struggle to effectively utilize retrieved visual knowledge for complex reasoning tasks, particularly those involving changes in perspective, scope, or occlusion, as demonstrated by the MRAG-Bench. Our llava-onevision-7b baseline model achieved a strong initial accuracy of 58.2%. We introduce an Object Detection Enhancement to the MRAG-Bench evaluation pipeline to push performance beyond the existing baseline by providing explicit visual grounding. This utilizes the DETR (DEtection TRansformer) model to analyze images and convert visual content (objects, counts, and spatial layout) into structured text descriptions. This structured analysis is used to create an Enhanced Prompt that guides the LLaVA model's reasoning.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Manaswini Talagadadivi</li>
          <li>Shen Ting Ang [&nbsp;<a href="https://linktr.ee/shenting">Linktree</a>&nbsp;]</li>
          <li>Huang Chao Ming, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/chao-ming-huang/">LinkedIn</a>&nbsp;]</li>
          <li>Benjamin Goh, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/benjamin-goh-45a0a7307">LinkedIn</a>&nbsp;]</li>
          <li>Estelle Sim, NUS SoC Undergraduate [&nbsp;<a href="http://www.linkedin.com/in/estelle-sim-862763385">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/mmrag-cs6101/mmrag-cs6101" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-07.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 7: Team 07 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-07.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-07.jpg" alt="Team 07 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 8: Team 09-->
    <tr id="project-2025-09">
      <td width="70%">
        <h1>
          Team 09: Automatic Accelerator Code Generation via LLM Agentic Workflow
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">This project explores the use of Large Language Models (LLMs) for automatic accelerator code generation and optimization.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">The project aims to improve performance for GPU by generating CUDA C++ code, targeting algorithms commonly used for Deep Learning. <br>
        The project utilizes VibeCodeHPC, an agentic LLM-based auto code generation system for High Performance Computing (HPC). It automates code generation and optimization through iterative prompt refinement and multi-agent collaboration.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Takanori Aoki [&nbsp;<a href="https://www.linkedin.com/in/takanori-aoki-7900a438/">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-09.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 8: Team 09 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-09.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-09.jpg" alt="Team 09 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 9: Team 10-->
    <tr id="project-2025-10">
      <td width="70%">
        <h1>
          Team 10: When Retrieval Misleads: Exploring Vulnerabilities in RAG
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">TODO<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Retrieval-Augmented Generation (RAG) has succeeded in knowledge-intensive tasks as it provides LLMs with external knowledge. However, existing works show that RAG can cause increased hallucination especially when the retrieved data involves counter-intuitive information. Therefore, we aim to study how different prompting and training techniques like chain-of-thought, retrieval-augmented fine-tuning, etc. can help mitigate these issues in small size language models.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Sahej Agarwal</li>
          <li>Jundong Xu</li>
          <li>Ruiwen Zhou</li>
          <li>Sahajpreet Singh</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-10.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 9: Team 10 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-10.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-10.jpg" alt="Team 10 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 10: Team 11-->
    <tr id="project-2025-11">
      <td width="70%">
        <h1>
          Team 11: Contexts Ground Discourse
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">We study the impacts of contexts for discourse understanding.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Contextual grounding is known to be useful for understanding discourse. However, there is a lack of precise evaluation of its specific benefits. In this project, we systematically study how context can improve the understanding of discourse relations between two arguments in the Wall Street Journal (WSJ) corpus, following the PDTB-style setting. We explore two types of context selection methods: (1) rule-based and (2) similarity-based. Our evaluation metric is our own DiSQ Score. Welcome to our poster session for more details!<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Yisong Miao</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-11.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 10: Team 11 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-11.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-11.jpg" alt="Team 11 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 11: Team 12-->
    <tr id="project-2025-12">
      <td width="70%">
        <h1>
          Team 12: Context-Aware RAG: Enhancing Retrieval with Contextual Information
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">TODO<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Retrieval-augmented generation (RAG) systems augmented with chain-of-thought (CoT) reasoning have achieved strong performance on multi-hop question answering, but they incur increased inference latency and produce lengthy contexts that hinder scalability. This project introduces two stopping criteria ‚Äî a Repetition-aware criterion that detects redundant reasoning tokens and halts generation when steps begin to repeat, and a Confidence-based criterion that terminates reasoning once model's confidence surpasses a threshold. We integrate these criteria into a CoT-enabled RAG pipeline and evaluate their feasibility on HotpotQA and 2WikiMultiHopQA, measuring inference latency, generated-context length, and answer quality. Rather than presupposing benefits, we report our experimental measurements and provide a detailed analysis of the observed advantages and limitations for each method. Our results offer grounded, practical insights into when lightweight stopping mechanisms may help make CoT-RAG systems more efficient and where further refinement is required.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Xinpeng Liu</li>
          <li>Ng Xuan Hern</li>
          <li>Oshan Jayawardena</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-12.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 11: Team 12 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-12.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-12.jpg" alt="Team 12 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 12: Team 13-->
    <tr id="project-2025-13">
      <td width="70%">
        <h1>
          Team 13: Improving Retrieval with Graph Pruning
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">GraphRAG systems enhance retrieval-augmented generation through knowledge graphs, but scale poorly due to retrieving too much, potentially clouding context due to the lost in the middle problem. We applied a systematic framework as outlined in PathRAG; to efficiently identify and select the most reliable relational path; to the communities relation based approach introduced by Microsoft in their implementation of GraphRAG as introduced in their paper GraphRAG Approach to Query-Focused Summarization. The objective is to retrieve with higher accuracy, reducing computational cost as well as compute time by allowing us to reduce the top K documents that should be retrieved.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">GraphRAG systems construct knowledge graphs from document collections to enhance retrieval-augmented generation, but these graphs can become computationally expensive and noisy at scale. Every additional node, edge, and community increases token usage, query latency, and risks introducing irrelevant context that may degrade answer quality.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Aaron Toh</li>
          <li>Frederick Amal Emerson</li>
          <li>Hong Yi</li>
          <li>Yong Ee</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-13.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 12: Team 13 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-13.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-13.jpg" alt="Team 13 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 13: Team 14-->
    <tr id="project-2025-14">
      <td width="70%">
        <h1>
          Team 14: A Unified Benchmark Suite for Retrieval-Augmented Speculative Decoding
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Retrieval-augmented speculative decoding has delivered promising gains in prior work, but thus far there has been no effort to make it production-ready. To our knowledge, we are the first to try.</p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Speculative decoding (SD) accelerates LLM inference by using a smaller model (drafter) to predict the next few tokens, then using a larger model (verifier) to verify all guesses in parallel. Recent work has explored allowing the drafter to retrieve from vector database as prior, giving rise to retrieval-augmented speculative decoding (RASD). While RASD has yielded promising results, lack of flexible configurability and the existence of multiple diverging techniques make it difficult to apply in practice. We propose a plug-and-play software framework for RASD that (a) allows convenient configuration of all parts of the RASD pipeline including drafter, verifier and vector database, and (b) reports useful metrics for evaluating throughput and generation quality.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Ada Ho, NUS SoC Undergraduate [ <a href="https://scallion3008.github.io/">Blog</a>, <a href="https://www.linkedin.com/in/scallion3008/">LinkedIn</a> ]</li>
          <li>Tan Chien Hao</li>
          <li>Lee Kwan Tze, NUS Undergraduate Student</li>
          <li>Benn Tan</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-14.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 13: Team 14 Poster-->
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-14.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-14.jpg" alt="Team 14 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
    <!-- ROW 14: Team 16-->
    <tr id="project-2025-16">
      <td width="70%">
        <h1>
          Team 16: Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">We uncover major latency in health-related Community Notes and propose CrowdNotes+, an LLM-augmented governance framework combining evidence-grounded note augmentation, utility-guided automation, and hierarchical evaluation. The result: more timely, factual, and effective crowd-sourced health misinformation responses.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), enables users to flag misleading posts, attach contextual notes, and vote on their helpfulness. However, our analysis of 30.8K health-related notes reveals significant latency, with a median delay of 17.6 hours before the first note receives a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified framework that leverages large language models (LLMs) to augment Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, along with a hierarchical three-step evaluation that progressively assesses relevance, correctness, and helpfulness. We instantiate the framework through HealthNotes, a benchmark of 1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness judge. Experiments on fifteen LLMs reveal an overlooked loophole in current helpfulness evaluation, where stylistic fluency is mistaken for factual accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented generation jointly enhance factual precision and evidence utility. These results point toward a hybrid human-AI governance model that improves both the rigor and timeliness of crowd-sourced fact-checking.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Jiaying Wu</li>
          <li>Zihang Fu</li>
          <li>Haonan Wang</li>
          <li>Fanxiao Li</li>
          <li>Min-Yen Kan</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2025/CS6101-16.jpg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 14: Team 16 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2025/CS6101-16.jpg" target="_blank">
          <img src="/cs6101/files/STePS-2025/CS6101-16.jpg" alt="Team 16 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to open PDF in a new window</p>
      </td>
    </tr>
  </tbody>
  
</table>


## Projects from Semester 2310 (AY 23/24, Sem I) featured at <a href="https://uvents.nus.edu.sg/event/23rd-steps">23th STePS</A>, held on 15 Nov 2023.

In AY23/24 Sem I, CS6101 was topically oriented on **[Large Language Models](https://uvents.nus.edu.sg/event/18th-steps/module/CS6101)**.  There were 41 students in 19 teams whose projects focused on recent research on topics related to LLMs.

<!--Table START-->
<table>
  <thead>
    <tr>
      <th><h1 align="center">Projects</h1></th>
      <th><h1 align="center">Posters</h1></th>
    </tr>
  </thead>
  <tbody>
    <!-- ROW 1: Team 02-->
    <tr id="project-2023-02">
      <td width="70%">
        <h1>
          Team 02: AugICL
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Augmented in-context learning for implicit discourse relation classification.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">We present an initial exploration of employing large language models (LLMs) for in-context learning (ICL) in the domain of implicit discourse relations classification. We analyze the effects of several ICL variables on task performance: (1) an inverted U-shaped relationship between the number of demonstrations and performance, (2) minimal impact from sample ordering, and (3) the benefits of incorporating diverse Level 2 sense samples. Additionally, we show that performance of the LLaMA-13B model can match that of the larger ChatGPT (175B) when augmenting with demonstrations akin to test samples.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Xiachong Feng</li>
          <li>Yisong Miao</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="/cs6101/files/STePS-2023/CS6101-02.png" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 1: Team 02 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2023/CS6101-02.png" target="_blank">
          <img src="/cs6101/files/STePS-2023/CS6101-02.png" alt="Team 02 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 2: Team 05-->
    <tr id="project-2023-05">
      <td width="70%">
        <h1>
          Team 05: Chain of Action
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Chain of Action with agents on code.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">In our work, we leverage advancements in agentic Large Language Models (LLMs) to enhance their skill decomposition and autonomous learning capabilities. We introduce a freeform action space, prompting the LLM to decompose complex problems into simpler tasks. Our hypothesis is that this approach will enable the LLM to incrementally build its skill set and strategically chain actions for problem-solving. Furthermore, we propose a novel retrieval-augmented learning framework, encouraging the model to acquire new skills from diverse prompts. This framework aims to enrich the LLM's knowledge base, allowing it to apply newly learned skills to solve the designated problem set effectively.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Nicholas Wong</li>
          <li>Richmond Sin</li>
          <li>Ellawela Suveen Thinusha Bandara</li>
          <li>Low Keng Hoong, Warren</li>
          <li>Gan Kah Ee</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://sites.google.com/view/chain-of-action/home" style="font-size:18px">Project Page</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2023/CS6101-05.jpeg" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 2: Team 05 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2023/CS6101-05.jpeg" target="_blank">
          <img src="/cs6101/files/STePS-2023/CS6101-05.jpeg" alt="Team 05 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 3: Team 06-->
    <tr id="project-2023-06">
      <td width="70%">
        <h1>
          Team 06: Exploring self-supervised webscraper code generation with LLMs
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Using LLMs to fully self-supervise the creation of a webscraper given a website.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">The process of writing code has been sped-up massively through the rapid adoption of LLMs in the programming process. However, most workflows still largely "human-in-the-loop", with a programmer running, verifying and helping to debug generated programs. We want to test the ability for LLMs to fully self-supervise in the code generation process: verifying if outputs are correct. We test this in the limited context of code generation for website scrapers and achieve a proof-of-concept that LLMs are able to fully self-supervise in the creation of webscrapers, enabling their use as a fully-automated generalized scraping tool.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Arnav Aggarwal</li>
          <li>Filbert Phang Kong San</li>
          <li>Soon Kang Le, Conrad</li>
          <li>Wee Yen Zhe</li>
          <li>Alson Jiang</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/cs6101-t2310-rc5/gptScraper" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2023/CS6101-06.png" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 3: Team 06 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2023/CS6101-06.png" target="_blank">
          <img src="/cs6101/files/STePS-2023/CS6101-06.png" alt="Team 06 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 4: Team 09-->
    <tr id="project-2023-09">
      <td width="70%">
        <h1>
          Team 09: LLM for Table Fact-Checking & Reasoning
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Evaluating the effectiveness of open-source LLM (e.g., LLama2) in table reasoning.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">As tables are a ubiquitous form of data presentation, ensuring their accuracy and reliability is important. This project is motivated by the need for robust table reasoning algorithms. We aim to evaluate the effectiveness of open-source LLM (e.g., LLama2) in this domain. Employing a range of benchmarks, we evaluate the models' accuracy in table reasoning and manually observe the error types. Our results aim to identify the strengths and weaknesses of open-resource LLMs in dealing with structured data, providing insights for future research and application.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Naomi Leow</li>
          <li>Xinyuan LU</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/XinyuanLu00/CS6101-TableFV" style="font-size:18px">GitHub Repository</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2023/CS6101-09.png" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 4: Team 09 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2023/CS6101-09.png" target="_blank">
          <img src="/cs6101/files/STePS-2023/CS6101-09.png" alt="Team 09 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 5: Team 14-->
    <tr id="project-2023-14">
      <td width="70%">
        <h1>
          Team 14: Textualization of Visual Information
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">A simple attempt to textualize images via vision experts and evaluate on <a href="https://arxiv.org/abs/2307.06281">MMBench</a>.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Visual information (image labels, image captions, object labels, etc.) contains the humans' (or visual model's) understanding and analysis of key features in the image. We investigate the performance of LLMs in directly accomplishing visual perception/reasoning tasks by transforming visual information in images into text via vision experts. We follow <a href="https://arxiv.org/abs/2306.16410">LENS</a> framework and evaluate on <a href="https://arxiv.org/abs/2307.06281">MMBench</a> and provide some insights.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Xiao Xu</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://uvents.nus.edu.sg/event/23rd-steps/module/CS6101/project/14" style="font-size:18px">Project Page</a>&nbsp;]
        [&nbsp;<a href="/cs6101/files/STePS-2023/CS6101-14.png" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 5: Team 14 Poster-->       
      <td width="30%">
        <a href="/cs6101/files/STePS-2023/CS6101-14.png" target="_blank">
          <img src="/cs6101/files/STePS-2023/CS6101-14.png" alt="Team 14 Poster" style="width: 100%; height: auto; border: 1px solid #ccc;" />
        </a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
  </tbody>
  
</table>


## Projects from Semester 2020 (AY 20/21, Sem II) featured at <a href="https://uvents.nus.edu.sg/event/18th-steps">18th STePS</A>, held on 14 Apr 2021.

In AY20/21 Sem II, CS6101 was topically oriented on **[Conversational Recommendation Systems](https://uvents.nus.edu.sg/event/18th-steps/module/CS6101)**.  There were 26 students in 10 teams whose projects focused on recent research on the topics of Conversational Systems, Recommender Systems and their intersections.

<!--Table START-->
<table>
  <thead>
    <tr>
      <th><h1 align="center">Projects</h1></th>
      <th><h1 align="center">Posters</h1></th>
    </tr>
  </thead>
  <tbody>
    <!-- ROW 1: Team 07-->
    <tr id="project-2021-07">
      <td width="70%">
        <h1>
          üèÜ 1st place, Team 07: Extending Neural Collaborative Filtering
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Extending the Neural Collaborative Filtering Framework to improve model understanding and robustness. Using additional Convolutional layers, Pairwise Loss Function and Auxiliary Information Embedding to explore potential model improvements.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">In our project, we explore the potential extensions to the Neural Collaborative Filtering (NCF) Framework to improve model understanding and robustness. Using additional Convolutional layers, Pairwise Loss Function and Auxiliary Information Embedding, we experiment with the MovieLens-1M dataset to attain better model performance on Hit Rate and NDCG metrics while attempting to improve model understanding through auxiliary embeddings.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Gabriel Loye, NUS SoC Undergraduate</li>
          <li>Clarence Ong, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/clarenceong97/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Nham Quoc Hung, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/quoc-hung-nham/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Sashankh CK, External Guest</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/gabrielloye/neural_collaborative_filtering" style="font-size:18px">Homepage</a>&nbsp;]
        [&nbsp;<a href="https://raw.githubusercontent.com/gabrielloye/neural_collaborative_filtering/master/assets/CS6101_Neural_Collaborative_Filtering_Poster.png" style="font-size:18px">Poster</a>&nbsp;]
      </td>
 <!-- ROW 1: Team 07 Poster-->       
      <td width="30%">
        <a href="https://raw.githubusercontent.com/gabrielloye/neural_collaborative_filtering/master/assets/CS6101_Neural_Collaborative_Filtering_Poster.png"><img src="https://raw.githubusercontent.com/gabrielloye/neural_collaborative_filtering/master/assets/CS6101_Neural_Collaborative_Filtering_Poster.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
 <!-- ROW 2: Team 04-->
    <tr id="project-2021-04">
      <td width="70%">
        <h1>
          ü•à 2nd place, Team 04: Causal Estimation for Conversational Recommender Systems
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">In this project, we study popularity bias in Recommender System (RecSys), Conversational Recsys, and their interplays.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">We discover (1) conversation can significantly mitigate popularity bias for traditional RecSys; (2) Conversation RecSys suffers from popularity bias itself. We propose a method to mitigate popularity bias in Conversational RecSys. Please refer to our poster for technical details. Our experiment is still WIP, we will update on this github repo: https://github.com/YisongMiao/cs6101 <BR/></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Yisong Miao, NUS Postgraduate Student  [&nbsp;<a href="https://yisong.me/">Personal Website</a>&nbsp;]</li>
          <li>Chenxin Wang, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/chengxin-wang-086304113/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/YisongMiao/cs6101">Homepage</a>&nbsp;]
        [&nbsp;<a href="https://yisong.me/publications/CausalEst-Poster.jpeg">Poster</a>&nbsp;]
      </td>
      <!--  ROW 2: Team 04 Poster-->
      <td width="30%">
        <a href="https://yisong.me/publications/CausalEst-Poster.jpeg">
        <img src="https://yisong.me/publications/CausalEst-Poster.jpeg" alt="Poster"/></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
        <!-- ROW 3: Team 09-->
    <tr id="project-2021-09">
      <td width="70%">
        <h1>
         ü•â 3rd place, Team 09: Beyond IGMC
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">We extend the state-of-the-art Inductive Graph Matrix Completion recommender system by introducing Graph Normalization and Layer Aggregation variants, and explore the models' potent transfer learning capabilities.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">In this project, we investigate how recent advances in Graph Neural Network models can impact and even improve the ability of the state-of-the-art Inductive Graph Matrix Completion (IGMC) recommender system to predict ratings in the setting of only having ratings of each user-item interaction. We show this through measuring the baseline model performance against the extensions using the RMSE scoring. <BR />
The IGMC is able to perform inductive matrix completion without any reliance on side-information. This allows the model to be highly applicable in many recommender system settings. It is also able to successfully transfer learning to other datasets with completely different recommender tasks and user bases.
We contribute 2 main extensions to the model, in particular: Graph-Normalisation and Layer Aggregation alternatives. We also extended the model visualisation and conducted meso-analysis on training examples with the greatest contributions to RMSE values. Furthermore, we explore the transfer learning capabilities of these inductive models, and benchmark the results against external datasets.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Stephen Tan, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/stephen-tan-hin-khai/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Axel Lau Wei En, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/axel-lau/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Joel Tan Wan Rong, NUS SoC Undergraduate [&nbsp;<a href="https://www.linkedin.com/in/joeltanwr/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Wendi Ren, External Guest</li>
          <li>Chan Guan Hao, External Guest</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://linktr.ee/BeyondIGMC">Homepage</a>&nbsp;]
        [&nbsp;<a href="https://www.youtube.com/watch?v=pEQxq7r-lgY">Video</a>&nbsp;]
        [&nbsp;<a href="https://drive.google.com/file/d/1Hso_rqGMXnJsRt1V9QmLUUGgbO6OcvZI/view">Poster</a>&nbsp;]]
      </td>
 <!-- ROW 3: Team 09 Poster-->       
      <td width="30%">
        <a href="https://drive.google.com/file/d/1Hso_rqGMXnJsRt1V9QmLUUGgbO6OcvZI/view">
        <img src="https://i.imgur.com/9k58lfq.png" /></a>
        <p align="center">Click the image to enlarge.</p>
      </td>
    </tr>
    <!-- ROW 4: Team 01-->
    <tr id="project-2021-01">
      <td width="70%">
        <h1>
          Team 01: Explore Multiple Response Modalities of DialogWAE
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">This project is focusing on assessing and interpreting the GMM prior components in DialogWAE.<BR/></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Neural response generation is a typical task in NLP community. DialogWAE is a new approach for dialogue modeling and response generation, which achieves SOTA result on popular datasets. In this work, we focus on exploring the various modalities of the generated responses. To be specific, we propose to: 
‚Ä¢	Analyze how the number K of prior components influences the overall performance.
‚Ä¢	Explore what each prior component of the Gaussian mixture distribution captures when K > 3.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Liu Ruofan, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/ruofanliu/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Liu Hongfu, NUS Postgraduate Student</li>
          <li>Liu Yong, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/yong-liu-b1037513/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/lindsey98/DialogWAE">Homepage</a>&nbsp;]
        [&nbsp;<a href="https://postimg.cc/sQN3hscm">Poster</a>&nbsp;]
      </td>
 <!-- ROW 4:  Team 01 Poster-->       
      <td width="30%">
        <a href="https://postimg.cc/sQN3hscm">
        <img src="https://i.imgur.com/EZwptz2.jpg" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 5: Team 02-->
    <tr id="project-2021-02">
      <td width="70%">
        <h1>
         Team 02: FiBiNET
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">This project is focusing on combining feature importance and bilinear feature interaction for click-through rate prediction<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">We explore potential extensions to one of the state-of-the-art recommender systems named FiBiNET which assigns importance to feature embeddings.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Qiao Rui, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/rui-qiao/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Sng Weicong, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/weicong-sng-42456ba8/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Li Zihan, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/zihan-li-nus/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://raw.githubusercontent.com/lizihan97/FiBiNET/main/FiBiNET.png">Poster</a>&nbsp;]
      </td>
 <!-- ROW 5: Team 02 Poster-->       
      <td width="30%">
        <a href="https://raw.githubusercontent.com/lizihan97/FiBiNET/main/FiBiNET.png"><img src="https://raw.githubusercontent.com/lizihan97/FiBiNET/main/FiBiNET.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>    
      <!-- ROW 6: Team 03-->
    <tr id="project-2021-03">
      <td width="70%">
        <h1>
         Team 03: Feedback-guided Preference Adaptation Network (FPAN)
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Multi-round conversational recommender systems (CRS), which interact with users by asking questions about attributes and recommending items multiple times in one conversation.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">FPAN uses gating modules to adapt the user embedding and item-level feedback, according to attribute-level feedback. This project looks to improve the offline and online training of FPAN. It does this by conducting a survey into the effectiveness of GraphSAGE convolutions. And, by introducing a function to calculate user & item bias.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Henry Kasim, NUS Podtgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/henrykasim/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Samuel Broughton, NUS Podtgraduate Student</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://drive.google.com/file/d/1SoAqjjmq48rMtvpQvCt-Wne5dXmiXOkA/view">Poster</a>&nbsp;]
      </td>
 <!-- ROW 6: Team 03 Poster-->       
      <td width="30%">
        <a href="https://drive.google.com/file/d/1SoAqjjmq48rMtvpQvCt-Wne5dXmiXOkA/view"><img src="https://i.imgur.com/m1pSi6v.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
    <!-- ROW 7: Team 05-->
    <tr id="project-2021-05">
      <td width="70%">
        <h1>
         Team 05: Counterfactual Recommender
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Relevance Matrix Factorization and Asymmetric Tri-training are employed to build a recommendation system. Its algorithm is evaluated by using Coat dataset to simulate a scenario that we have only biased observational data for model training while evaluating on unbiased data.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Implicit feedback is easy to collect and can be useful to build a recommendation system in online service. However, the feedback suffered from popularity bias because a user gives feedback to an item only if it is exposed. To build an unbiased recommendation system, counterfactual learning and meta learning approaches are applied to deal with such observational data.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Aadit Rahul Kamat, NUS Undergraduate Student [&nbsp;<a href="https://www.linkedin.com/in/aaditkamat/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>  Takanori Aoki, External Guest </li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://drive.google.com/file/d/1nZ40KXyccOxUJRD53c0m2VcWtBiVWkBb/view">Poster</a>&nbsp;]
      </td>
 <!-- ROW 7: Team 05 Poster-->       
      <td width="30%">
        <a href="https://drive.google.com/file/d/1nZ40KXyccOxUJRD53c0m2VcWtBiVWkBb/view"><img src="https://i.imgur.com/Tw3ywe8.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
     <!-- ROW 8: Team 06-->
    <tr id="project-2021-06">
      <td width="70%">
        <h1>
         Team 06: Diversifying Dialogue Generation with Non-Conversational Text
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Implementation for the paper Diversifying Dialogue Generation with Non-Conversational Text on English.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">Traditional neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low diversity problem when it comes to open domain dialogue generation. The authors aim to diversify the dialogue generation with non-conversational text corpus in Chinese language. We attempt to extend this work to conversational and non-conversational datasets in English Analysis on how filtering the non-conversational corpus based on topic affects the result (selected topics: Politics, Attitude & Emotion, Health).<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Yeo Qi Xun, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/yeo-qi-xun-8975a114b/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Yuxi Xie, NUS Postgraduate Student [&nbsp;<a href="https://www.linkedin.com/in/yuxi-xie-494265181/">LinkedIn</a>&nbsp;]</li>
          <li>Tian Zhen, NUS Postgraduate Student </li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/YuxiXie/Diversifying-Dialogue-Generation-with-Non-Conversational-Text">Homepage</a>&nbsp;]
        [&nbsp;<a href="https://www.youtube.com/watch?v=PWOr1ay1CNM">Video</a>&nbsp;]
        [&nbsp;<a href="https://i.imgur.com/J3tzCR5.png">Poster</a>&nbsp;]
      </td>
 <!-- ROW 8: Team 06 Poster-->       
      <td width="30%">
        <a href="https://i.imgur.com/J3tzCR5.png"><img src="https://i.imgur.com/J3tzCR5.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
       <!-- ROW 9: Team 08-->
    <tr id="project-2021-08">
      <td width="70%">
        <h1>
         Team 08: NN for Ad Recommendation
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">The project extends the popular DeepFM neural network to better predict users' click-through-rate of advertisements.<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">DeepFM is a popular click-through-rate (CTR) model developed by Huawei's Noah's Ark Lab. In this project, we modifies the original neural network structure for better CTR predictions. More specifically, we introduced pooling layers to better capture the higher order feature interactions and b. added a linear layer to assign weights to the constituent deep model and factorisation machine (FM) model when combining the outputs. Our experimental results show that both extensions are able to improve the accuracy of the original DeepFM model.<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Muhammad Assyarul Ariffin Bin Omar, NUS Undergraduate Student [&nbsp;<a href="https://www.linkedin.com/in/muhd-assyarul-ariffin-bin-omar/?originalSubdomain=sg">LinkedIn</a>&nbsp;]</li>
          <li>Lee Xiong An, External Guest</li>
          <li>Xu Pengtai, External Guest</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://i.ibb.co/vVzJdLq/WING-Poster-STe-PS-2021-6101-08.jpg">Poster</a>&nbsp;]
      </td>
 <!-- ROW 9: Team 08 Poster-->       
      <td width="30%">
        <a href="https://i.ibb.co/vVzJdLq/WING-Poster-STe-PS-2021-6101-08.jpg"><img src="https://i.ibb.co/vVzJdLq/WING-Poster-STe-PS-2021-6101-08.jpg" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
       <!-- ROW 10: Team 10-->
    <tr id="project-2021-10">
      <td width="70%">
        <h1>
         Team 10: KGRecSys
        </h1>
        <h2>üìñ Abstract </h2>
        <p style="font-size:18px">Implementation of the paper "KGAT: Knowledge Graph Attention Network for Recommendation".<BR /></p>
        <h2>‚úçÔ∏è Description </h2>
        <p style="font-size:18px">In this project, we implement the KGAT model as described in the paper "KGAT: Knowledge Graph Attention Network for Recommendation".<BR /></p>
        <h2>‚òÄÔ∏è Team Member </h2>
        <ul style="font-size:18px">
          <li>Evan Chong, External Guest</li>
          <li>Rabiul Awal, External Guest</li>
        </ul>
        <h2>üìª Media Links</h2>
        [&nbsp;<a href="https://github.com/evantkchong/cs6101_kgrecsys">Homepage</a>&nbsp;]
[&nbsp;<a href="https://raw.githubusercontent.com/evantkchong/cs6101_kgrecsys/main/docs/KGRecSys_poster_STePS_2021.png">Poster</a>&nbsp;]
      </td>
 <!-- ROW 10: Team 10 Poster-->       
      <td width="30%">
        <a href="https://postimg.cc/9RPrbz7y"><img src="https://i.imgur.com/PvASl0J.png" /></a>
        <p align="center">Click the image to enlarge</p>
      </td>
    </tr>
  </tbody>
</table>
